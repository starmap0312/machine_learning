# large margin classification

# cost function
  the cost function uses a function with values similar to sigmoid function, but composed of two straight-lines
  if y = 1, then the cost = 0 for z >= 1
  if y = 0, then the cost = 0 for z <= -1

# regularization parameter
  by convention, it uses the inverse of the regularization parameter of logistic regression
  the outlier examples is likely to be ignored for not too large regularization parameter C

# non-linear decision boundary
  the features are transformed into another space
  all the example points in the original feature space are treated as origins of new feature space
  we have lower cost if an example is close to examples with higher weights

# need to specify
  1) choice of parameter C
  2) choice of kernels
     if no kernel, it is called linear kernel

# logistic regression vs. SVMs
  n: number of features
  m: number of training examples
  1) if n is large (relative to m)
     ex. n >= m, n = 10,000, m = 1,000
     if there are many features but few examples
     use logistic regression or SVM with linear kernel (no kernel) would probably do fine
  2) if n is small, m is intermediate
     ex. n = 1,000, m = 10,000
     use SVM with Gaussian kernel would probably do well
  3) if n is small, m is large
     ex. n = 1,000, m = 100,000
     using SVM would be computational expensive
     create/add more features and then use logistic regerssion or SVM with linear kernel (no kernel)
  note:
    neural network is likely to work well for all of the above settings, but may be slow to train
