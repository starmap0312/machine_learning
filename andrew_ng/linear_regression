# linear regression
  used to predict a value (ex. predict the price of a house based on its size)
  1) hypothesis: predictions for any given feature (feature x -> prediction y)
  2) parameters: parameters of terms in a hypothesis 
  3) cost function: errors against the data for any given hypothesis (a higher-order function parameterized for all hypotheses)
  4) objective/goal: find the optimal parameters with the minimum error

# the cost function
  how far the predictions are away from the actual data (how good your line fits the data)
  a way of measuring how well any given hypothesis fits into the data
  ex. square error function, or mean squared error (most commonly used cost function for linear regression, a convex function, bowl-shaped)

# gradient descent
  parameter learning: used to find the optimal parameters (the best line that fits the data)
  1) learning rate: step size when descending (it affects when the convergence will happen)
  2) derivative:

# feature scaling
  make the features are on a similar scale, so that the gradient converges more quickly
    ex. -1 <= x_i <= 1           (fine)
        -3 <= x_i <= 3           (fine)
    ex. -0.0001 <= x_i <= 0.0001 (poor scaled)
        -10000 <= x_i <= 10000   (poor scaled)
  mean normalization 
    replace x_i with x_i - mean_i, so that each feature has zero mean
  standard deviation
    replace x_i with (x_i - mean_i) / (max - min), so that each feature has zero mean and range 1 (i.e. zero standard deviation)

# learning rate
  how to make gradient descent works correctly
    evaluate and plot the cost function for 100 iterations, and then 200 iterations (the cost should decrease after each iteration)
      from the plotted figure, we can see if the gradient descent has converged
    define a convergence test
      declare convergence if the cost descreases by less than 0.001 (epsilon) in one iteration
  how to choose learning rate
    for sufficiently small learning rate, the cost should descrease on every iteration
    1) if the learning rate is too small, we have slow convergence
    2) if the learning rate is too large, the cost may not descrease on every iteration, so it may not converge
       if the cost is increasing after each iteration, choose a smaller learning rate
       if the cost is decreasing and increasing alternately, choose a smaller learning rate
    choose learning rate:
      first, choose in a rate of 10, ex. ..., 0.001, 0.01, 0.1, 1, ...
      next, we may find that 0.01 (too small) and 0.1 (too large), so we choose in a rate of 3, ex. 0.01, 0.03, 0.09, 0.1

# polynomial regression
  define new features based on what insight you have to a problem (fit more complex (ex. quadratic/cubic) function to your data, instead of just linear function) 
    ex. suppose we have features
          width and height
        we can define a new feature
          area = width * height
        sometimes this leads to a better training model
  polynomial regression
    ex. qudratic model: add new features of the square or existing features
        so that we have a qudratic fit of the data
  note that feature scaling becomes increasingly important, because without scaling the gradient may converge slowly 
    ex. size  : 1 - 1,000
        size^2: 1 - 1,000,000
  choice of features
    ex. 1, size, size ^ 2
        1, size, size ^ (1 / 2)
    there are algorithms that automatically choose the features for you

# normal equation
  1) no need to choose the learning rate
  2) no iteration
  3) need to compute an inverse matrix: O(n ^ 3)
  4) it is slow if the number of features is large
     if number of features <= 10,000, we could consider to use normal equation
     otherwise, use gradient descent or other optimization algoirthms
  5) it does not work for complex models, ex. logistic regression
  6) it does not work for non-invertible matrix
     ex. redundant features (linear dependent)
         size in feet ^ 2
         size in meter ^ 2
     ex. too many features: number of examples <= number of features
         delete some features, or use regularization
